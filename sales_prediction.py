# -*- coding: utf-8 -*-
"""sales_prediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12YBVgclCrcY2fI70o2LkY7VoQN8iBdAO
"""

#!pip install pandas numpy seaborn matplotlib klib dtale scikit-learn joblib pandas-profiling

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
# %matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns

df_train= pd.read_csv(r'/content/Train.csv')
df_test= pd.read_csv(r'/content/Test.csv')

df_train.head()

#df_test

df_train.shape

df_train.isnull().sum()

df_test.isnull().sum()

df_train.info()

df_train.describe()

df_train['Item_Weight'].describe()

df_train['Item_Weight'].fillna(df_train['Item_Weight'].mean(),inplace=True)
df_test['Item_Weight'].fillna(df_test['Item_Weight'].mean(),inplace=True)

df_train.isnull().sum()

df_train['Item_Weight'].describe()

df_train['Outlet_Size'].value_counts()

df_train['Outlet_Size'].mode()

df_train['Outlet_Size'].fillna(df_train['Outlet_Size'].mode()[0],inplace=True)
df_test['Outlet_Size'].fillna(df_test['Outlet_Size'].mode()[0],inplace=True)

df_train.isnull().sum()

df_test.isnull().sum()

df_train.drop(['Item_Identifier','Outlet_Identifier'],axis=1,inplace=True)
df_test.drop(['Item_Identifier','Outlet_Identifier'],axis=1,inplace=True)

df_train

!pip install dtale

import dtale

dtale.show(df_train)

!pip install pandas-profiling  # Install the missing pandas_profiling module

!pip install pydantic-settings  # Make sure pydantic-settings is installed
!export PYDANTIC_SETTINGS_VERSION=2  # Tell pydantic to use the pydantic-settings package

!pip install --upgrade pandas-profiling
!pip install --upgrade pydantic pydantic-settings

!pip uninstall pandas-profiling
!pip install pandas-profiling

!pip install pydantic==1.10

!pip install pandas-profiling==3.5.0

!pip install --upgrade numba

!pip install numba==0.55.2

!pip install visions==0.7.4

!pip install pandas-profiling==3.5.0
!pip install visions==0.7.4
!pip install numba==0.55.2
!pip install pydantic==1.10

!python -m venv myenv
!source myenv/bin/activate  # On Windows use `myenv\Scripts\activate`
!pip install pandas-profiling

import pandas_profiling
import numba
import visions

print("Pandas Profiling:", pandas_profiling.__version__)
print("Numba:", numba.__version__)
print("Visions:", visions.__version__)

from pandas_profiling import ProfileReport

profile = ProfileReport(df_train, title="Pandas Profiling Report")

profile

plt.figure(figsize=(10,5))
sns.heatmap(df_train.corr(),annot=True)
plt.show()

!pip install klib

import klib

# klib.describe - functions for visualizing datasets
klib.cat_plot(df_train) # returns a visualization of the number and frequency of categorical features

klib.corr_mat(df_train) # returns a color-encoded correlation matrix

klib.corr_plot(df_train) # returns a color-encoded heatmap, ideal for correlations

klib.dist_plot(df_train) # returns a distribution plot for every numeric feature

klib.missingval_plot(df_train) # returns a figure containing information about missing values

# klib.clean - functions for cleaning datasets
klib.data_cleaning(df_train) # performs datacleaning (drop duplicates & empty rows/cols, adjust dtypes,...)

klib.clean_column_names(df_train) # cleans and standardizes column names, also called inside data_cleaning()

df_train.info()

df_train=klib.convert_datatypes(df_train) # converts existing to more efficient dtypes, also called inside data_cleaning()
df_train.info()

klib.mv_col_handling(df_train)

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()

df_train['item_fat_content']= le.fit_transform(df_train['item_fat_content'])
df_train['item_type']= le.fit_transform(df_train['item_type'])
df_train['outlet_size']= le.fit_transform(df_train['outlet_size'])
df_train['outlet_location_type']= le.fit_transform(df_train['outlet_location_type'])
df_train['outlet_type']= le.fit_transform(df_train['outlet_type'])

df_train

X=df_train.drop('item_outlet_sales',axis=1)

Y=df_train['item_outlet_sales']

from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X,Y, random_state=101, test_size=0.2)

X.describe()

from sklearn.preprocessing import StandardScaler
sc= StandardScaler()

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer

# Identify numerical and categorical columns
numerical_cols = X_train.select_dtypes(include=['float', 'int']).columns
categorical_cols = X_train.select_dtypes(include=['object']).columns

# Create transformers
numeric_transformer = StandardScaler()
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

# Combine transformers
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

# Fit and transform the training data
X_train_std = preprocessor.fit_transform(X_train)

# Transform the test data
X_test_std = preprocessor.transform(X_test)

X_train_std

X_test_std

Y_train

Y_train

import joblib

joblib.dump(sc,r'D:\BigMart-Sales-Prediction-using-Machine-Learning-main (1)\BigMart-Sales-Prediction-using-Machine-Learning-main\sc.sav')

from sklearn.linear_model import LinearRegression
lr= LinearRegression()

import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression

# Assuming X_train_std is a pandas DataFrame
imputer = SimpleImputer(strategy='mean')  # Replace missing values with the mean of the column
X_train_imputed = imputer.fit_transform(X_train_std)

lr = LinearRegression()
lr.fit(X_train_imputed, Y_train)  # Use the imputed data for training

X_test.head()

import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression

# Assuming X_train_std is a pandas DataFrame
imputer = SimpleImputer(strategy='mean')  # Replace missing values with the mean of the column
X_train_imputed = imputer.fit_transform(X_train_std)

lr = LinearRegression()
lr.fit(X_train_imputed, Y_train)  # Use the imputed data for training

# Impute missing values in X_test_std using the same imputer
X_test_imputed = imputer.transform(X_test_std)  # Use the fitted imputer to transform the test data

# Now predict using the imputed test data
Y_pred_lr = lr.predict(X_test_imputed)

from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

print(r2_score(Y_test,Y_pred_lr))
print(mean_absolute_error(Y_test,Y_pred_lr))
print(np.sqrt(mean_squared_error(Y_test,Y_pred_lr)))

joblib.dump(lr,r'D:\Python37\Project\BigMart-Sales\models\lr.sav')

from sklearn.ensemble import RandomForestRegressor
rf= RandomForestRegressor(n_estimators=1000)

import numpy as np
from sklearn.impute import SimpleImputer

# Create an imputer object
imputer = SimpleImputer(strategy='mean')  # Replace 'mean' with other strategies if needed

# Fit the imputer to your training data and transform it
X_train_std_imputed = imputer.fit_transform(X_train_std)

# Now fit the RandomForestRegressor on the imputed data
rf.fit(X_train_std_imputed, Y_train)

# Transform the test data using the same imputer fitted on training data
X_test_std_imputed = imputer.transform(X_test_std)

# Now predict using the imputed test data
Y_pred_rf = rf.predict(X_test_std_imputed)

print(r2_score(Y_test,Y_pred_rf))
print(mean_absolute_error(Y_test,Y_pred_rf))
print(np.sqrt(mean_squared_error(Y_test,Y_pred_rf)))

import numpy as np
from sklearn.impute import SimpleImputer
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor

# define models and parameters
model = RandomForestRegressor()
n_estimators = [10, 100, 1000]
max_depth=range(1,31)
min_samples_leaf=np.linspace(0.1, 1.0)
max_features=["auto", "sqrt", "log2"]
min_samples_split=np.linspace(0.1, 1.0, 10)

# define grid search
grid = dict(n_estimators=n_estimators)

#cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=101)

# Impute missing values before fitting the model
imputer = SimpleImputer(strategy='mean')
X_train_std_imputed = imputer.fit_transform(X_train_std)

grid_search_forest = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1,
                           scoring='r2',error_score=0,verbose=2,cv=2)

# Fit the model on imputed data
grid_search_forest.fit(X_train_std_imputed, Y_train)

# summarize results
print(f"Best: {grid_search_forest.best_score_:.3f} using {grid_search_forest.best_params_}")
means = grid_search_forest.cv_results_['mean_test_score']
stds = grid_search_forest.cv_results_['std_test_score']
params = grid_search_forest.cv_results_['params']

for mean, stdev, param in zip(means, stds, params):
    print(f"{mean:.3f} ({stdev:.3f}) with: {param}")

grid_search_forest.best_params_

grid_search_forest.best_score_

# Impute missing values in the test data using the same imputer fitted on the training data
X_test_std_imputed = imputer.transform(X_test_std)

# Predict using the imputed test data
Y_pred_rf_grid = grid_search_forest.predict(X_test_std_imputed)

r2_score(Y_test,Y_pred_rf_grid)

import joblib

joblib.dump(grid_search_forest,r'D:\Python37\Projects\ML_BigMart Sales Prediction\models\random_forest_grid.sav')

model=joblib.load(r'D:\Python37\Projects\ML_BigMart Sales Prediction\models\random_forest_grid.sav')

import nbformat
import json

# Replace 'your_notebook_name.ipynb' with the actual name of your notebook file
notebook_path = '/content/sales_prediction.ipynb'

try:
    with open(notebook_path, 'r') as f:
        notebook_content = nbformat.read(f, as_version=4)

    # Iterate through each cell and remove 'metadata.widgets' if it exists
    for cell in notebook_content.cells:
        if 'metadata' in cell and 'widgets' in cell['metadata']:
            del cell['metadata']['widgets']

    # Save the modified notebook
    with open(notebook_path, 'w') as f:
        nbformat.write(notebook_content, f)

    print(f"Successfully removed 'metadata.widgets' from '{notebook_path}'")

except FileNotFoundError:
    print(f"Error: Notebook file not found at '{notebook_path}'")
except Exception as e:
    print(f"An error occurred: {e}")